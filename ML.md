## Attention

+ attention can be described as mapping a query and a set of key-value pairs to an output

https://www.cnblogs.com/robert-dlut/p/8638283.html

+ self-attention

+ transformer

+ n-gram

+ c-bow

+ softmax

+ i am vegitable
