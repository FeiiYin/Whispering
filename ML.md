## Attention

+ attention & self attention & transformer

can be described as mapping a query and a set of key-value pairs to an output

https://www.cnblogs.com/robert-dlut/p/8638283.html

https://zhuanlan.zhihu.com/p/53682800

+ 概率图模型 & HMM & CRF

https://zhuanlan.zhihu.com/p/55238865

+ 最大似然估计

https://zhuanlan.zhihu.com/p/26614750

+ 协方差矩阵

+ BPE

+ n-gram

+ c-bow

+ softmax

+ i am vegitable
