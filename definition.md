## some thing

+ 白化

https://www.cnblogs.com/demian/p/7627324.html

+ p, np, npc, np-hard

http://www.matrix67.com/blog/archives/105







## Machine learning

+ 信息

  信息是用来消除随机不确定性的东西。

  + 越不可能的事件发生了，我们获取到的信息量就越大（显然:)）

​		考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。

​		时间概率为$p$时，其不确定性为
$$
f(p)=-log(p)
$$

+ 信息熵

  信源的平均不确定性，（信息量的期望），称为信息熵

$$
H(U)=E[f(p)]=-\sum_{i=1}^{n}{p_ilog(p_i)}
$$

​		一个系统越是有序，信息熵就越低，信息熵也可以说是系统**有序化程度**的一个度量。

+ 条件熵

  已知随机变量X的条件下随机变量Y的不确定性。指的是期望，而非X为定值。
  $$
  H(Y|X)=-\sum_x\sum_yp(x,y)log(p(y|x))
  $$
  
+ 信息增益

  指信息熵的有效减少量，信息熵 - 条件熵，决策树中使用

+ 相对熵

  相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。

  + 即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。

  + 值越小，表示q分布和p分布越接近

  $$
  D_{KL}(p||q)=\sum_{i=1}^{n}{p_ilog(\frac{p_i}{q_i})}
  $$

+ 交叉熵
  $$
  H(p,q)=-\sum_{i=1}^{n}{p_ilog({q_i})}\\
  D_{KL}=-H(熵)+H(p,q)
  $$
  实际前一部分不变，因为都是p，真实值，所以只关注后部分的交叉熵

+ 多分类

  loss = sum(loss)

referemce: 

<https://www.cnblogs.com/yinheyi/p/6843009.html>

<https://blog.csdn.net/tsyccnh/article/details/79163834>



#### 决策树

---

**决策树学习的损失函数通常是 正则化的极大似然函数**。

决策树学习的目标是 以损失函数为目标函数的最小化。

+ 停止条件：1、子节点只有一种类型，（容易过拟合，不采用）2、节点数或迭代数达到某阈值
+ 现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

ID3算法对于缺失值的情况没有考虑。

+ 信息增益比

  用于C4.5
  $$
  GainRatio(D,A)=\frac{信息增益}{训练数据集的经验熵}
  $$

+ 

$$
Gini(p)=1-\sum{p_k^2}
$$

​	用于Cart

reference:

<https://shuwoom.com/?p=1452>



#### 随机森林

---

每棵树的训练样本从训练集中随机且有放回的抽取

抗噪，防止过拟合

自身训练集，即是一个无偏估计，直接使用类似k折交叉验证



#### 贝叶斯

---

极大似然估计

把概率连乘，转换成对数，乘法变加法，且防止了参数下溢

最大后验概率
$$
P(c|x)=\prod{P(c|x_i)}
$$


拉普拉斯平滑，+1

基于贝叶斯公式来估计后验概率，假设属性相互独立。由训练集给定。

+ 贝叶斯网：假设属性对其他属性有依赖，做成图。
  $$
  P(NOW)=P(NOW|parent)
  $$
  构建可以分类

reference: <https://blog.csdn.net/yangjingjing9/article/details/79986371>





